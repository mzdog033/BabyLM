{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPh95pbV4rPhbjlzCiq7LIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzdog033/BabyLM/blob/main/Newest_BabyLM_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov2Tl4DQ2loq",
        "outputId": "1cf60581-1d16-4eb6-fa53-e7b905de55be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/baseline-pretraining-main.zip\n",
            "replace baseline-pretraining-main/.gitignore? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/baseline-pretraining-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers ipdb datasets jax==0.3.21 flax==0.6.1 sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "994KV2Fc2q0j",
        "outputId": "bcb4d714-e381-4eb9-a015-7c6546a24344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipdb\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jax==0.3.21\n",
            "  Downloading jax-0.3.21.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxlib==0.3.20\n",
            "  Downloading jaxlib-0.3.20-cp310-cp310-manylinux2014_x86_64.whl (72.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flax==0.6.1\n",
            "  Downloading flax-0.6.1-py3-none-any.whl (185 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.6/185.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (1.22.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (1.10.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (4.6.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from jax==0.3.21) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flax==0.6.1) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.6.1) (1.0.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax==0.6.1) (0.1.5)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.6.1) (13.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.6.1) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.6.1) (3.0.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->jax==0.3.21) (5.12.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->jax==0.3.21) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax==0.6.1) (2.8.2)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax==0.6.1) (0.1.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax==0.6.1) (0.1.8)\n",
            "INFO: pip is looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting chex>=0.1.5 (from optax->flax==0.6.1)\n",
            "  Downloading chex-0.1.81-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading chex-0.1.6-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax==0.6.1) (0.12.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.6.1) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->flax==0.6.1) (1.16.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.3.21-py3-none-any.whl size=1263549 sha256=770fcf0c60aec02b4604d6701da136f34973dc964a6a5f7023f969486190f455\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/c2/44/ee513466f2ce03e7da29a79154423677b7d382869053b2d7dd\n",
            "Successfully built jax\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, jedi, dill, multiprocess, jaxlib, huggingface-hub, transformers, jax, ipdb, datasets, chex, flax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.10+cuda11.cudnn86\n",
            "    Uninstalling jaxlib-0.4.10+cuda11.cudnn86:\n",
            "      Successfully uninstalled jaxlib-0.4.10+cuda11.cudnn86\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.10\n",
            "    Uninstalling jax-0.4.10:\n",
            "      Successfully uninstalled jax-0.4.10\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.7\n",
            "    Uninstalling chex-0.1.7:\n",
            "      Successfully uninstalled chex-0.1.7\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.6.11\n",
            "    Uninstalling flax-0.6.11:\n",
            "      Successfully uninstalled flax-0.6.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.2.6 requires jax>=0.4.9, but you have jax 0.3.21 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chex-0.1.6 datasets-2.13.1 dill-0.3.6 flax-0.6.1 huggingface-hub-0.16.2 ipdb-0.13.13 jax-0.3.21 jaxlib-0.3.20 jedi-0.18.2 multiprocess-0.70.14 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxlib==0.3.20+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVtaNFA492tp",
        "outputId": "021a673b-f21f-48db-fc44-c33ecdf86ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting jaxlib==0.3.20+cuda11.cudnn82\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.20%2Bcuda11.cudnn82-cp310-cp310-manylinux2014_x86_64.whl (162.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.2/162.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.3.20+cuda11.cudnn82) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.3.20+cuda11.cudnn82) (1.22.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from jaxlib==0.3.20+cuda11.cudnn82) (1.4.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.20\n",
            "    Uninstalling jaxlib-0.3.20:\n",
            "      Successfully uninstalled jaxlib-0.3.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.2.6 requires jax>=0.4.9, but you have jax 0.3.21 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jaxlib-0.3.20+cuda11.cudnn82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/baseline-pretraining-main/scripts/train_t5_babylm.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1moivJix3vGq",
        "outputId": "fd4f6000-30b3-4159-dca9-18ac3dc878af"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-06 13:03:20.691212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-e7d89630ea2c7e3e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "100% 2/2 [00:00<00:00, 230.79it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-e7d89630ea2c7e3e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-ca382a97d1586837.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-e7d89630ea2c7e3e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-91053762343bd9af.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-e7d89630ea2c7e3e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-18923c063f85c6af.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-e7d89630ea2c7e3e/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-08b9268d1eb2abfe.arrow\n",
            "2023-07-06 13:03:24.933094: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Epoch ... :   0% 0/1 [00:00<?, ?it/s]\n",
            "Training...:   0% 0/182 [00:00<?, ?it/s]\u001b[A\n",
            "Training...:   1% 1/182 [02:37<7:54:53, 157.42s/it]\u001b[A\n",
            "Training...:   1% 2/182 [02:38<3:16:09, 65.38s/it] \u001b[A\n",
            "Training...:   2% 3/182 [02:39<1:47:17, 35.97s/it]\u001b[A\n",
            "Training...:   2% 4/182 [02:40<1:05:42, 22.15s/it]\u001b[A\n",
            "Training...:   3% 5/182 [02:41<42:47, 14.51s/it]  \u001b[A\n",
            "Training...:   3% 6/182 [02:42<29:02,  9.90s/it]\u001b[A\n",
            "Training...:   4% 7/182 [02:43<20:21,  6.98s/it]\u001b[A\n",
            "Training...:   4% 8/182 [02:44<14:41,  5.06s/it]\u001b[A\n",
            "Training...:   5% 9/182 [02:45<10:54,  3.78s/it]\u001b[A\n",
            "Training...:   5% 10/182 [02:46<08:20,  2.91s/it]\u001b[A\n",
            "Training...:   6% 11/182 [02:47<06:35,  2.31s/it]\u001b[A\n",
            "Training...:   7% 12/182 [02:47<05:23,  1.90s/it]\u001b[A\n",
            "Training...:   7% 13/182 [02:48<04:32,  1.62s/it]\u001b[A\n",
            "Training...:   8% 14/182 [02:49<03:58,  1.42s/it]\u001b[A\n",
            "Training...:   8% 15/182 [02:50<03:34,  1.28s/it]\u001b[A\n",
            "Training...:   9% 16/182 [02:51<03:17,  1.19s/it]\u001b[A\n",
            "Training...:   9% 17/182 [02:52<03:05,  1.12s/it]\u001b[A\n",
            "Training...:  10% 18/182 [02:53<02:56,  1.08s/it]\u001b[A\n",
            "Training...:  10% 19/182 [02:54<02:50,  1.05s/it]\u001b[A\n",
            "Training...:  11% 20/182 [02:55<02:46,  1.03s/it]\u001b[A\n",
            "Training...:  12% 21/182 [02:56<02:43,  1.01s/it]\u001b[A\n",
            "Training...:  12% 22/182 [02:57<02:40,  1.00s/it]\u001b[A\n",
            "Training...:  13% 23/182 [02:58<02:39,  1.00s/it]\u001b[A\n",
            "Training...:  13% 24/182 [02:59<02:37,  1.00it/s]\u001b[A\n",
            "Training...:  14% 25/182 [03:00<02:36,  1.00it/s]\u001b[A\n",
            "Training...:  14% 26/182 [03:01<02:35,  1.00it/s]\u001b[A\n",
            "Training...:  15% 27/182 [03:02<02:34,  1.00it/s]\u001b[A\n",
            "Training...:  15% 28/182 [03:03<02:33,  1.00it/s]\u001b[A\n",
            "Training...:  16% 29/182 [03:04<02:33,  1.00s/it]\u001b[A\n",
            "Training...:  16% 30/182 [03:05<02:32,  1.01s/it]\u001b[A\n",
            "Training...:  17% 31/182 [03:06<02:32,  1.01s/it]\u001b[A\n",
            "Training...:  18% 32/182 [03:07<02:32,  1.01s/it]\u001b[A\n",
            "Training...:  18% 33/182 [03:08<02:31,  1.02s/it]\u001b[A\n",
            "Training...:  19% 34/182 [03:09<02:30,  1.02s/it]\u001b[A\n",
            "Training...:  19% 35/182 [03:10<02:30,  1.02s/it]\u001b[A\n",
            "Training...:  20% 36/182 [03:11<02:29,  1.02s/it]\u001b[A\n",
            "Training...:  20% 37/182 [03:12<02:28,  1.02s/it]\u001b[A\n",
            "Training...:  21% 38/182 [03:13<02:27,  1.03s/it]\u001b[A\n",
            "Training...:  21% 39/182 [03:14<02:27,  1.03s/it]\u001b[A\n",
            "Training...:  22% 40/182 [03:15<02:26,  1.03s/it]\u001b[A\n",
            "Training...:  23% 41/182 [03:17<02:25,  1.03s/it]\u001b[A\n",
            "Training...:  23% 42/182 [03:18<02:24,  1.03s/it]\u001b[A\n",
            "Training...:  24% 43/182 [03:19<02:23,  1.03s/it]\u001b[A\n",
            "Training...:  24% 44/182 [03:20<02:22,  1.03s/it]\u001b[A\n",
            "Training...:  25% 45/182 [03:21<02:21,  1.04s/it]\u001b[A\n",
            "Training...:  25% 46/182 [03:22<02:20,  1.04s/it]\u001b[A\n",
            "Training...:  26% 47/182 [03:23<02:19,  1.04s/it]\u001b[A\n",
            "Training...:  26% 48/182 [03:24<02:18,  1.03s/it]\u001b[A\n",
            "Training...:  27% 49/182 [03:25<02:17,  1.03s/it]\u001b[A\n",
            "Training...:  27% 50/182 [03:26<02:15,  1.03s/it]\u001b[A\n",
            "Training...:  28% 51/182 [03:27<02:14,  1.03s/it]\u001b[A\n",
            "Training...:  29% 52/182 [03:28<02:12,  1.02s/it]\u001b[A\n",
            "Training...:  29% 53/182 [03:29<02:11,  1.02s/it]\u001b[A\n",
            "Training...:  30% 54/182 [03:30<02:09,  1.02s/it]\u001b[A\n",
            "Training...:  30% 55/182 [03:31<02:08,  1.01s/it]\u001b[A\n",
            "Training...:  31% 56/182 [03:32<02:07,  1.01s/it]\u001b[A\n",
            "Training...:  31% 57/182 [03:33<02:05,  1.01s/it]\u001b[A\n",
            "Training...:  32% 58/182 [03:34<02:04,  1.01s/it]\u001b[A\n",
            "Training...:  32% 59/182 [03:35<02:03,  1.00s/it]\u001b[A\n",
            "Training...:  33% 60/182 [03:36<02:02,  1.00s/it]\u001b[A\n",
            "Training...:  34% 61/182 [03:37<02:00,  1.00it/s]\u001b[A\n",
            "Training...:  34% 62/182 [03:38<01:59,  1.00it/s]\u001b[A\n",
            "Training...:  35% 63/182 [03:39<01:58,  1.01it/s]\u001b[A\n",
            "Training...:  35% 64/182 [03:40<01:56,  1.01it/s]\u001b[A\n",
            "Training...:  36% 65/182 [03:41<01:55,  1.01it/s]\u001b[A\n",
            "Training...:  36% 66/182 [03:42<01:54,  1.01it/s]\u001b[A\n",
            "Training...:  37% 67/182 [03:43<01:53,  1.01it/s]\u001b[A\n",
            "Training...:  37% 68/182 [03:44<01:52,  1.02it/s]\u001b[A\n",
            "Training...:  38% 69/182 [03:45<01:50,  1.02it/s]\u001b[A\n",
            "Training...:  38% 70/182 [03:46<01:49,  1.02it/s]\u001b[A\n",
            "Training...:  39% 71/182 [03:47<01:48,  1.02it/s]\u001b[A\n",
            "Training...:  40% 72/182 [03:48<01:47,  1.02it/s]\u001b[A\n",
            "Training...:  40% 73/182 [03:49<01:46,  1.03it/s]\u001b[A\n",
            "Training...:  41% 74/182 [03:50<01:45,  1.03it/s]\u001b[A\n",
            "Training...:  41% 75/182 [03:51<01:44,  1.03it/s]\u001b[A\n",
            "Training...:  42% 76/182 [03:52<01:42,  1.03it/s]\u001b[A\n",
            "Training...:  42% 77/182 [03:53<01:41,  1.03it/s]\u001b[A\n",
            "Training...:  43% 78/182 [03:53<01:40,  1.03it/s]\u001b[A\n",
            "Training...:  43% 79/182 [03:54<01:39,  1.03it/s]\u001b[A\n",
            "Training...:  44% 80/182 [03:55<01:38,  1.03it/s]\u001b[A\n",
            "Training...:  45% 81/182 [03:56<01:37,  1.04it/s]\u001b[A\n",
            "Training...:  45% 82/182 [03:57<01:36,  1.04it/s]\u001b[A\n",
            "Training...:  46% 83/182 [03:58<01:35,  1.03it/s]\u001b[A\n",
            "Training...:  46% 84/182 [03:59<01:34,  1.04it/s]\u001b[A\n",
            "Training...:  47% 85/182 [04:00<01:33,  1.04it/s]\u001b[A\n",
            "Training...:  47% 86/182 [04:01<01:32,  1.04it/s]\u001b[A\n",
            "Training...:  48% 87/182 [04:02<01:31,  1.04it/s]\u001b[A\n",
            "Training...:  48% 88/182 [04:03<01:30,  1.04it/s]\u001b[A\n",
            "Training...:  49% 89/182 [04:04<01:29,  1.04it/s]\u001b[A\n",
            "Training...:  49% 90/182 [04:05<01:28,  1.04it/s]\u001b[A\n",
            "Training...:  50% 91/182 [04:06<01:27,  1.04it/s]\u001b[A\n",
            "Training...:  51% 92/182 [04:07<01:26,  1.04it/s]\u001b[A\n",
            "Training...:  51% 93/182 [04:08<01:25,  1.04it/s]\u001b[A\n",
            "Training...:  52% 94/182 [04:09<01:24,  1.04it/s]\u001b[A\n",
            "Training...:  52% 95/182 [04:10<01:23,  1.04it/s]\u001b[A\n",
            "Training...:  53% 96/182 [04:11<01:22,  1.04it/s]\u001b[A\n",
            "Training...:  53% 97/182 [04:12<01:21,  1.04it/s]\u001b[A\n",
            "Training...:  54% 98/182 [04:13<01:21,  1.03it/s]\u001b[A\n",
            "Training...:  54% 99/182 [04:14<01:20,  1.03it/s]\u001b[A\n",
            "Training...:  55% 100/182 [04:15<01:19,  1.03it/s]\u001b[A\n",
            "Training...:  55% 101/182 [04:16<01:18,  1.03it/s]\u001b[A\n",
            "Training...:  56% 102/182 [04:17<01:17,  1.03it/s]\u001b[A\n",
            "Training...:  57% 103/182 [04:18<01:16,  1.03it/s]\u001b[A\n",
            "Training...:  57% 104/182 [04:19<01:15,  1.03it/s]\u001b[A\n",
            "Training...:  58% 105/182 [04:20<01:14,  1.04it/s]\u001b[A\n",
            "Training...:  58% 106/182 [04:20<01:13,  1.04it/s]\u001b[A\n",
            "Training...:  59% 107/182 [04:21<01:12,  1.04it/s]\u001b[A\n",
            "Training...:  59% 108/182 [04:22<01:10,  1.04it/s]\u001b[A\n",
            "Training...:  60% 109/182 [04:23<01:09,  1.04it/s]\u001b[A\n",
            "Training...:  60% 110/182 [04:24<01:09,  1.04it/s]\u001b[A\n",
            "Training...:  61% 111/182 [04:26<01:13,  1.04s/it]\u001b[A\n",
            "Training...:  62% 112/182 [04:26<01:10,  1.01s/it]\u001b[A\n",
            "Training...:  62% 113/182 [04:27<01:08,  1.01it/s]\u001b[A\n",
            "Training...:  63% 114/182 [04:28<01:06,  1.02it/s]\u001b[A\n",
            "Training...:  63% 115/182 [04:29<01:05,  1.03it/s]\u001b[A\n",
            "Training...:  64% 116/182 [04:30<01:04,  1.03it/s]\u001b[A\n",
            "Training...:  64% 117/182 [04:31<01:03,  1.03it/s]\u001b[A\n",
            "Training...:  65% 118/182 [04:32<01:02,  1.03it/s]\u001b[A\n",
            "Training...:  65% 119/182 [04:33<01:01,  1.03it/s]\u001b[A\n",
            "Training...:  66% 120/182 [04:34<01:00,  1.03it/s]\u001b[A\n",
            "Training...:  66% 121/182 [04:35<00:59,  1.03it/s]\u001b[A\n",
            "Training...:  67% 122/182 [04:36<00:58,  1.03it/s]\u001b[A\n",
            "Training...:  68% 123/182 [04:37<00:57,  1.03it/s]\u001b[A\n",
            "Training...:  68% 124/182 [04:38<00:56,  1.03it/s]\u001b[A\n",
            "Training...:  69% 125/182 [04:39<00:55,  1.03it/s]\u001b[A\n",
            "Training...:  69% 126/182 [04:40<00:54,  1.03it/s]\u001b[A\n",
            "Training...:  70% 127/182 [04:41<00:53,  1.03it/s]\u001b[A\n",
            "Training...:  70% 128/182 [04:42<00:52,  1.03it/s]\u001b[A\n",
            "Training...:  71% 129/182 [04:43<00:51,  1.03it/s]\u001b[A\n",
            "Training...:  71% 130/182 [04:44<00:50,  1.03it/s]\u001b[A\n",
            "Training...:  72% 131/182 [04:45<00:49,  1.03it/s]\u001b[A\n",
            "Training...:  73% 132/182 [04:46<00:48,  1.03it/s]\u001b[A\n",
            "Training...:  73% 133/182 [04:47<00:47,  1.03it/s]\u001b[A\n",
            "Training...:  74% 134/182 [04:48<00:46,  1.03it/s]\u001b[A\n",
            "Training...:  74% 135/182 [04:49<00:45,  1.03it/s]\u001b[A\n",
            "Training...:  75% 136/182 [04:50<00:44,  1.03it/s]\u001b[A\n",
            "Training...:  75% 137/182 [04:51<00:43,  1.03it/s]\u001b[A\n",
            "Training...:  76% 138/182 [04:52<00:42,  1.03it/s]\u001b[A\n",
            "Training...:  76% 139/182 [04:53<00:41,  1.03it/s]\u001b[A\n",
            "Training...:  77% 140/182 [04:54<00:40,  1.03it/s]\u001b[A\n",
            "Training...:  77% 141/182 [04:55<00:39,  1.03it/s]\u001b[A\n",
            "Training...:  78% 142/182 [04:56<00:38,  1.03it/s]\u001b[A\n",
            "Training...:  79% 143/182 [04:57<00:37,  1.03it/s]\u001b[A\n",
            "Training...:  79% 144/182 [04:57<00:36,  1.03it/s]\u001b[A\n",
            "Training...:  80% 145/182 [04:58<00:35,  1.03it/s]\u001b[A\n",
            "Training...:  80% 146/182 [04:59<00:34,  1.03it/s]\u001b[A\n",
            "Training...:  81% 147/182 [05:00<00:33,  1.03it/s]\u001b[A\n",
            "Training...:  81% 148/182 [05:01<00:32,  1.03it/s]\u001b[A\n",
            "Training...:  82% 149/182 [05:02<00:32,  1.03it/s]\u001b[A\n",
            "Training...:  82% 150/182 [05:03<00:31,  1.03it/s]\u001b[A\n",
            "Training...:  83% 151/182 [05:04<00:30,  1.03it/s]\u001b[A\n",
            "Training...:  84% 152/182 [05:05<00:28,  1.04it/s]\u001b[A\n",
            "Training...:  84% 153/182 [05:06<00:28,  1.04it/s]\u001b[A\n",
            "Training...:  85% 154/182 [05:07<00:27,  1.04it/s]\u001b[A\n",
            "Training...:  85% 155/182 [05:08<00:26,  1.04it/s]\u001b[A\n",
            "Training...:  86% 156/182 [05:09<00:25,  1.04it/s]\u001b[A\n",
            "Training...:  86% 157/182 [05:10<00:24,  1.04it/s]\u001b[A\n",
            "Training...:  87% 158/182 [05:11<00:23,  1.04it/s]\u001b[A\n",
            "Training...:  87% 159/182 [05:12<00:22,  1.04it/s]\u001b[A\n",
            "Training...:  88% 160/182 [05:13<00:21,  1.04it/s]\u001b[A\n",
            "Training...:  88% 161/182 [05:14<00:20,  1.04it/s]\u001b[A\n",
            "Training...:  89% 162/182 [05:15<00:19,  1.04it/s]\u001b[A\n",
            "Training...:  90% 163/182 [05:16<00:18,  1.04it/s]\u001b[A\n",
            "Training...:  90% 164/182 [05:17<00:17,  1.04it/s]\u001b[A\n",
            "Training...:  91% 165/182 [05:18<00:16,  1.04it/s]\u001b[A\n",
            "Training...:  91% 166/182 [05:19<00:15,  1.04it/s]\u001b[A\n",
            "Training...:  92% 167/182 [05:20<00:14,  1.04it/s]\u001b[A\n",
            "Training...:  92% 168/182 [05:21<00:13,  1.04it/s]\u001b[A\n",
            "Training...:  93% 169/182 [05:22<00:12,  1.04it/s]\u001b[A\n",
            "Training...:  93% 170/182 [05:23<00:11,  1.05it/s]\u001b[A\n",
            "Training...:  94% 171/182 [05:23<00:10,  1.05it/s]\u001b[A\n",
            "Training...:  95% 172/182 [05:24<00:09,  1.05it/s]\u001b[A\n",
            "Training...:  95% 173/182 [05:25<00:08,  1.05it/s]\u001b[A\n",
            "Training...:  96% 174/182 [05:26<00:07,  1.05it/s]\u001b[A\n",
            "Training...:  96% 175/182 [05:27<00:06,  1.05it/s]\u001b[A\n",
            "Training...:  97% 176/182 [05:28<00:05,  1.05it/s]\u001b[A\n",
            "Training...:  97% 177/182 [05:29<00:04,  1.05it/s]\u001b[A\n",
            "Training...:  98% 178/182 [05:30<00:03,  1.05it/s]\u001b[A\n",
            "Training...:  98% 179/182 [05:31<00:02,  1.04it/s]\u001b[A\n",
            "Training...:  99% 180/182 [05:32<00:01,  1.04it/s]\u001b[A\n",
            "Training...:  99% 181/182 [05:33<00:01,  1.02s/it]\u001b[A\n",
            "Training...: 100% 182/182 [05:34<00:00,  1.84s/it]\n",
            "Epoch ... : 100% 1/1 [05:34<00:00, 334.69s/it]\n"
          ]
        }
      ]
    }
  ]
}